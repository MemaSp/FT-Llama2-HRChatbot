{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c211477b1b4e4521a93be2528ff5a82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = '40960MB'  # It's cleaner to not use an f-string here since it's a static value\n",
    "\n",
    "    # Load model without specifying device_map initially\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        max_memory={i: max_memory for i in range(n_gpus)}  # Configure maximum memory per GPU\n",
    "    )\n",
    "\n",
    "    # Manual device mapping example\n",
    "    if n_gpus > 1:\n",
    "        # Distribute model layers manually across available GPUs\n",
    "        device_map = {i: list(range(i * len(model.transformer.h) // n_gpus,\n",
    "                                     (i + 1) * len(model.transformer.h) // n_gpus))\n",
    "                      for i in range(n_gpus)}\n",
    "        model.parallelize(device_map)\n",
    "    else:\n",
    "        # If there's only one GPU, use it directly\n",
    "        model.to('cuda:0')\n",
    "\n",
    "    # Load tokenizer and set special tokens as needed\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Needed for LLaMA tokenizer\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        \n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Path to the local dataset file\n",
    "local_dataset_path = r'C:\\Users\\spite\\Documents\\FT-Llama2-HR_Chatbot\\Dataset_3_Narrative_HR_Time_Tracking_Dataset.json'\n",
    "\n",
    "# Load the dataset from the local file\n",
    "dataset = load_dataset('json', data_files={'train': local_dataset_path}, split='train')\n",
    "\n",
    "# Now you can use 'dataset' as you would with any dataset loaded from the Hugging Face hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# New instruction dataset\n",
    "dolly_dataset = \"databricks/databricks-dolly-15k\"\n",
    "\n",
    "\n",
    "dataset = load_dataset(dolly_dataset, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3rd Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# New instruction dataset\n",
    "drop_dataset = \"ucinlp/drop\"\n",
    "\n",
    "dataset = load_dataset(drop_dataset, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 43\n",
      "Column names are: ['category', 'instruction', 'context', 'response']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dataset = databricks-dolly-15k & custom/mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction', 'context', 'response')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    \n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dataset = drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('passage', 'question', 'answers_spans')\n",
    "    Then concatenate them using two newline characters\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is a passage followed by a question. Write a response that appropriately answers the question.\"\n",
    "    INPUT_KEY = \"### Passage:\"\n",
    "    QUESTION_KEY = \"### Question:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    # Extract answer from answer spans or default to an empty string\n",
    "    answer_text = sample['answers_spans'] if sample['answers_spans'] else 'No answer provided'\n",
    "\n",
    "    # Build the parts of the prompt using the sample data\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    passage = f\"{INPUT_KEY}\\n{sample['passage']}\"\n",
    "    question = f\"{QUESTION_KEY}\\n{sample['question']}\"\n",
    "    answer = f\"{RESPONSE_KEY}\\n{answer_text}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    # Only include parts that are not None\n",
    "    parts = [part for part in [blurb, passage, question, answer, end] if part]\n",
    "    \n",
    "    # Join all parts into the final formatted prompt\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    \n",
    "    # Assign the formatted prompt back to the sample under a new key\n",
    "    sample[\"formatted_text\"] = formatted_prompt\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer, max_length, seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\"\"\"\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove the original fields\n",
    "    def _preprocessing_function(batch):\n",
    "        return tokenizer(\n",
    "            batch['formatted_text'],\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        #remove_columns=['passage', 'question', 'answers_spans'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BitsAndBytesConfig -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LoRa configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        \n",
    "    )\n",
    "     \n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train \n",
    " pre-process the dataset and load the model using the set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6348f224a387423086d1d5984627ec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:732: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\" \n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94902cd5059f47db823143366f43b18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041a023e2a0043c991737243c924e4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Preprocess dataset\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "seed=0\n",
    "\n",
    "dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop Training - not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import prepare_model_for_kbit_training, find_all_linear_names, create_peft_config, get_peft_model\n",
    "from peft.utils import print_trainable_parameters\n",
    "\n",
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            max_steps=50,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "     \n",
    "    do_train = True\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def loop_train(model, tokenizer, dataset, output_base_dir, num_iterations=10):\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        output_dir = f\"{output_base_dir}/iteration_{i}\"\n",
    "        print(f\"\\nStarting iteration {i}/{num_iterations}...\")\n",
    "        train(model, tokenizer, dataset, output_dir)\n",
    "\n",
    "# Example usage:\n",
    "# Replace these with your actual model, tokenizer, and dataset\n",
    "model = model\n",
    "tokenizer = tokenizer\n",
    "dataset = dataset\n",
    "\n",
    "output_base_dir = \"results/llama2\"\n",
    "loop_train(model, tokenizer, dataset, output_base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OG TRaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all params: 3,540,389,888 || trainable params: 39,976,960 || trainable%: 1.1291682911958425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 302387200 0.08541070604255438\n",
      "torch.uint8 3238002688 0.9145892939574456\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39834ac83a74b9db9b8d2d4eed3b481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\spite\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:670: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4229, 'grad_norm': 2.650351047515869, 'learning_rate': 0.0001, 'epoch': 0.09}\n",
      "{'loss': 2.3617, 'grad_norm': 2.8774495124816895, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 2.1073, 'grad_norm': 2.1688761711120605, 'learning_rate': 0.00019705882352941177, 'epoch': 0.28}\n",
      "{'loss': 1.693, 'grad_norm': 2.9910287857055664, 'learning_rate': 0.00019411764705882354, 'epoch': 0.37}\n",
      "{'loss': 1.2782, 'grad_norm': 3.3001697063446045, 'learning_rate': 0.0001911764705882353, 'epoch': 0.47}\n",
      "{'loss': 0.9754, 'grad_norm': 1.5782090425491333, 'learning_rate': 0.00018823529411764707, 'epoch': 0.56}\n",
      "{'loss': 0.8417, 'grad_norm': 2.714700222015381, 'learning_rate': 0.00018529411764705883, 'epoch': 0.65}\n",
      "{'loss': 0.7186, 'grad_norm': 2.1657679080963135, 'learning_rate': 0.0001823529411764706, 'epoch': 0.74}\n",
      "{'loss': 0.6776, 'grad_norm': 1.6456832885742188, 'learning_rate': 0.00017941176470588236, 'epoch': 0.84}\n",
      "{'loss': 0.6848, 'grad_norm': 6.898524761199951, 'learning_rate': 0.00017647058823529413, 'epoch': 0.93}\n",
      "{'loss': 0.6857, 'grad_norm': 8.606111526489258, 'learning_rate': 0.0001735294117647059, 'epoch': 1.02}\n",
      "{'loss': 0.579, 'grad_norm': 2.180655002593994, 'learning_rate': 0.00017058823529411766, 'epoch': 1.12}\n",
      "{'loss': 0.4179, 'grad_norm': 4.23138952255249, 'learning_rate': 0.00016764705882352942, 'epoch': 1.21}\n",
      "{'loss': 0.4965, 'grad_norm': 2.8815417289733887, 'learning_rate': 0.0001647058823529412, 'epoch': 1.3}\n",
      "{'loss': 0.3751, 'grad_norm': 1.0100867748260498, 'learning_rate': 0.00016176470588235295, 'epoch': 1.4}\n",
      "{'loss': 0.4071, 'grad_norm': 1.6584477424621582, 'learning_rate': 0.0001588235294117647, 'epoch': 1.49}\n",
      "{'loss': 0.409, 'grad_norm': 1.2627118825912476, 'learning_rate': 0.00015588235294117648, 'epoch': 1.58}\n",
      "{'loss': 0.3307, 'grad_norm': 0.7676533460617065, 'learning_rate': 0.00015294117647058822, 'epoch': 1.67}\n",
      "{'loss': 0.4497, 'grad_norm': 1.0701904296875, 'learning_rate': 0.00015000000000000001, 'epoch': 1.77}\n",
      "{'loss': 0.367, 'grad_norm': 0.9847206473350525, 'learning_rate': 0.00014705882352941178, 'epoch': 1.86}\n",
      "{'loss': 0.2756, 'grad_norm': 0.6400859951972961, 'learning_rate': 0.00014411764705882354, 'epoch': 1.95}\n",
      "{'loss': 0.478, 'grad_norm': 1.2096704244613647, 'learning_rate': 0.0001411764705882353, 'epoch': 2.05}\n",
      "{'loss': 0.3406, 'grad_norm': 0.9676517844200134, 'learning_rate': 0.00013823529411764707, 'epoch': 2.14}\n",
      "{'loss': 0.3243, 'grad_norm': 0.6709452867507935, 'learning_rate': 0.00013529411764705884, 'epoch': 2.23}\n",
      "{'loss': 0.2449, 'grad_norm': 0.6624411940574646, 'learning_rate': 0.0001323529411764706, 'epoch': 2.33}\n",
      "{'loss': 0.254, 'grad_norm': 0.5654590129852295, 'learning_rate': 0.00012941176470588237, 'epoch': 2.42}\n",
      "{'loss': 0.2658, 'grad_norm': 0.7204999327659607, 'learning_rate': 0.0001264705882352941, 'epoch': 2.51}\n",
      "{'loss': 0.3714, 'grad_norm': 0.8852997422218323, 'learning_rate': 0.0001235294117647059, 'epoch': 2.6}\n",
      "{'loss': 0.2979, 'grad_norm': 0.8862280249595642, 'learning_rate': 0.00012058823529411765, 'epoch': 2.7}\n",
      "{'loss': 0.4322, 'grad_norm': 1.0979547500610352, 'learning_rate': 0.00011764705882352942, 'epoch': 2.79}\n",
      "{'loss': 0.271, 'grad_norm': 0.7760190367698669, 'learning_rate': 0.00011470588235294118, 'epoch': 2.88}\n",
      "{'loss': 0.3201, 'grad_norm': 0.8344491720199585, 'learning_rate': 0.00011176470588235294, 'epoch': 2.98}\n",
      "{'loss': 0.2044, 'grad_norm': 0.587584376335144, 'learning_rate': 0.0001088235294117647, 'epoch': 3.07}\n",
      "{'loss': 0.2281, 'grad_norm': 0.7208171486854553, 'learning_rate': 0.00010588235294117647, 'epoch': 3.16}\n",
      "{'loss': 0.2954, 'grad_norm': 0.6550241708755493, 'learning_rate': 0.00010294117647058823, 'epoch': 3.26}\n",
      "{'loss': 0.202, 'grad_norm': 0.6392906308174133, 'learning_rate': 0.0001, 'epoch': 3.35}\n",
      "{'loss': 0.2244, 'grad_norm': 0.6589193344116211, 'learning_rate': 9.705882352941177e-05, 'epoch': 3.44}\n",
      "{'loss': 0.2245, 'grad_norm': 0.634524941444397, 'learning_rate': 9.411764705882353e-05, 'epoch': 3.53}\n",
      "{'loss': 0.2652, 'grad_norm': 1.0096741914749146, 'learning_rate': 9.11764705882353e-05, 'epoch': 3.63}\n",
      "{'loss': 0.2333, 'grad_norm': 0.6027031540870667, 'learning_rate': 8.823529411764706e-05, 'epoch': 3.72}\n",
      "{'loss': 0.2476, 'grad_norm': 0.6905355453491211, 'learning_rate': 8.529411764705883e-05, 'epoch': 3.81}\n",
      "{'loss': 0.2458, 'grad_norm': 0.876940131187439, 'learning_rate': 8.23529411764706e-05, 'epoch': 3.91}\n",
      "{'loss': 0.1888, 'grad_norm': 0.8119058609008789, 'learning_rate': 7.941176470588235e-05, 'epoch': 4.0}\n",
      "{'loss': 0.1908, 'grad_norm': 0.6196363568305969, 'learning_rate': 7.647058823529411e-05, 'epoch': 4.09}\n",
      "{'loss': 0.1519, 'grad_norm': 0.6456071734428406, 'learning_rate': 7.352941176470589e-05, 'epoch': 4.19}\n",
      "{'loss': 0.1606, 'grad_norm': 0.5503711700439453, 'learning_rate': 7.058823529411765e-05, 'epoch': 4.28}\n",
      "{'loss': 0.1615, 'grad_norm': 0.7816437482833862, 'learning_rate': 6.764705882352942e-05, 'epoch': 4.37}\n",
      "{'loss': 0.1566, 'grad_norm': 0.7577654123306274, 'learning_rate': 6.470588235294118e-05, 'epoch': 4.47}\n",
      "{'loss': 0.1536, 'grad_norm': 0.7884191274642944, 'learning_rate': 6.176470588235295e-05, 'epoch': 4.56}\n",
      "{'loss': 0.1651, 'grad_norm': 0.670625627040863, 'learning_rate': 5.882352941176471e-05, 'epoch': 4.65}\n",
      "{'loss': 0.1813, 'grad_norm': 0.7747513651847839, 'learning_rate': 5.588235294117647e-05, 'epoch': 4.74}\n",
      "{'loss': 0.217, 'grad_norm': 1.180370569229126, 'learning_rate': 5.294117647058824e-05, 'epoch': 4.84}\n",
      "{'loss': 0.1741, 'grad_norm': 0.9943494200706482, 'learning_rate': 5e-05, 'epoch': 4.93}\n",
      "{'loss': 0.1876, 'grad_norm': 1.1987663507461548, 'learning_rate': 4.705882352941177e-05, 'epoch': 5.02}\n",
      "{'loss': 0.1633, 'grad_norm': 0.6547397375106812, 'learning_rate': 4.411764705882353e-05, 'epoch': 5.12}\n",
      "{'loss': 0.1368, 'grad_norm': 0.5906378626823425, 'learning_rate': 4.11764705882353e-05, 'epoch': 5.21}\n",
      "{'loss': 0.1496, 'grad_norm': 0.6860017776489258, 'learning_rate': 3.8235294117647055e-05, 'epoch': 5.3}\n",
      "{'loss': 0.1456, 'grad_norm': 0.9378007650375366, 'learning_rate': 3.529411764705883e-05, 'epoch': 5.4}\n",
      "{'loss': 0.1282, 'grad_norm': 0.6608718633651733, 'learning_rate': 3.235294117647059e-05, 'epoch': 5.49}\n",
      "{'loss': 0.1276, 'grad_norm': 0.7894709706306458, 'learning_rate': 2.9411764705882354e-05, 'epoch': 5.58}\n",
      "{'loss': 0.1396, 'grad_norm': 0.6313635110855103, 'learning_rate': 2.647058823529412e-05, 'epoch': 5.67}\n",
      "{'loss': 0.1459, 'grad_norm': 0.7442564964294434, 'learning_rate': 2.3529411764705884e-05, 'epoch': 5.77}\n",
      "{'loss': 0.1129, 'grad_norm': 0.5501082539558411, 'learning_rate': 2.058823529411765e-05, 'epoch': 5.86}\n",
      "{'loss': 0.1165, 'grad_norm': 0.6163260340690613, 'learning_rate': 1.7647058823529414e-05, 'epoch': 5.95}\n",
      "{'loss': 0.119, 'grad_norm': 0.6569036841392517, 'learning_rate': 1.4705882352941177e-05, 'epoch': 6.05}\n",
      "{'loss': 0.1063, 'grad_norm': 0.6458547115325928, 'learning_rate': 1.1764705882352942e-05, 'epoch': 6.14}\n",
      "{'loss': 0.1186, 'grad_norm': 0.6186901926994324, 'learning_rate': 8.823529411764707e-06, 'epoch': 6.23}\n",
      "{'loss': 0.1289, 'grad_norm': 0.7770562171936035, 'learning_rate': 5.882352941176471e-06, 'epoch': 6.33}\n",
      "{'loss': 0.1221, 'grad_norm': 0.789004385471344, 'learning_rate': 2.9411764705882355e-06, 'epoch': 6.42}\n",
      "{'loss': 0.1112, 'grad_norm': 0.5794291496276855, 'learning_rate': 0.0, 'epoch': 6.51}\n",
      "{'train_runtime': 506.0655, 'train_samples_per_second': 0.553, 'train_steps_per_second': 0.138, 'train_loss': 0.40980453065463474, 'epoch': 6.51}\n",
      "***** train metrics *****\n",
      "  epoch                    =       6.51\n",
      "  train_loss               =     0.4098\n",
      "  train_runtime            = 0:08:26.06\n",
      "  train_samples_per_second =      0.553\n",
      "  train_steps_per_second   =      0.138\n",
      "{'train_runtime': 506.0655, 'train_samples_per_second': 0.553, 'train_steps_per_second': 0.138, 'train_loss': 0.40980453065463474, 'epoch': 6.51}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            max_steps=70,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "     \n",
    "    do_train = True\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "output_dir = \"results/llama2/final_checkpoint\"\n",
    "train(model, tokenizer, dataset, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " save to a new directory and associated tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\",offload_buffers=True, torch_dtype=torch.bfloat16, offload_folder='./offload_folder')\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = \"results/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\n",
    "# save tokenizer for easy inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a32c839da94986869deff0eb535648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "output_merged_dir = \"results/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"results/llama2/final_checkpoint\",  # Make sure this path is correct\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        offload_buffers=True,\n",
    "        offload_folder='./offload_folder'\n",
    "    )\n",
    "\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    if n_gpus > 1:\n",
    "        device_map = {\n",
    "            i: list(range(i * len(model.transformer.h) // n_gpus, (i + 1) * len(model.transformer.h) // n_gpus))\n",
    "            for i in range(n_gpus)\n",
    "        }\n",
    "        model.parallelize(device_map)\n",
    "    else:\n",
    "        model.to('cuda:0')\n",
    "\n",
    "    model = model.merge_and_unload()\n",
    "    model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\n",
    "    tokenizer_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    tokenizer.save_pretrained(output_merged_dir)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(\"RuntimeError:\", e)\n",
    "    if \"CUDA out of memory\" in str(e):\n",
    "        print(\"Trying to free up CUDA memory\")\n",
    "        torch.cuda.empty_cache()\n",
    "        # Consider reducing batch size or using more GPUs\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clears cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
