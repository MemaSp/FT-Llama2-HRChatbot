{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a6e9ed7d9440a0b92578dc5d8609e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{40960}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        \n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Path to the local dataset file\n",
    "local_dataset_path = r'C:\\Users\\spite\\Documents\\FT-Llama2-HR_Chatbot\\Dataset_1_Narrative_HR_Time_Tracking_Dataset.json'\n",
    "\n",
    "# Load the dataset from the local file\n",
    "dataset = load_dataset('json', data_files={'train': local_dataset_path}, split='train')\n",
    "\n",
    "# Now you can use 'dataset' as you would with any dataset loaded from the Hugging Face hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# New instruction dataset\n",
    "dolly_dataset = \"databricks/databricks-dolly-15k\"\n",
    "\n",
    "\n",
    "dataset = load_dataset(dolly_dataset, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3rd Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# New instruction dataset\n",
    "drop_dataset = \"ucinlp/drop\"\n",
    "\n",
    "dataset = load_dataset(drop_dataset, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 15011\n",
      "Column names are: ['instruction', 'context', 'response', 'category']\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of prompts: {len(dataset)}')\n",
    "print(f'Column names are: {dataset.column_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dataset = databricks-dolly-15k & custom/mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction', 'context', 'response')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['context']}\" if sample[\"context\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['response']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    \n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"context\", \"response\", \"text\", \"category\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dataset = drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('passage', 'question', 'answers_spans')\n",
    "    Then concatenate them using two newline characters\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is a passage followed by a question. Write a response that appropriately answers the question.\"\n",
    "    INPUT_KEY = \"### Passage:\"\n",
    "    QUESTION_KEY = \"### Question:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    # Extract answer from answer spans or default to an empty string\n",
    "    answer_text = sample['answers_spans'] if sample['answers_spans'] else 'No answer provided'\n",
    "\n",
    "    # Build the parts of the prompt using the sample data\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    passage = f\"{INPUT_KEY}\\n{sample['passage']}\"\n",
    "    question = f\"{QUESTION_KEY}\\n{sample['question']}\"\n",
    "    answer = f\"{RESPONSE_KEY}\\n{answer_text}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    # Only include parts that are not None\n",
    "    parts = [part for part in [blurb, passage, question, answer, end] if part]\n",
    "    \n",
    "    # Join all parts into the final formatted prompt\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    \n",
    "    # Assign the formatted prompt back to the sample under a new key\n",
    "    sample[\"formatted_text\"] = formatted_prompt\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer, max_length, seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\"\"\"\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove the original fields\n",
    "    def _preprocessing_function(batch):\n",
    "        return tokenizer(\n",
    "            batch['formatted_text'],\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        #remove_columns=['passage', 'question', 'answers_spans'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BitsAndBytesConfig -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LoRa configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    \"\"\"\n",
    "    Create Parameter-Efficient Fine-Tuning config for your model\n",
    "    :param modules: Names of the modules to apply Lora to\n",
    "    \"\"\"\n",
    "    offload_folder='./'\n",
    "    \n",
    "    config = LoraConfig(\n",
    "        r=16,  # dimension of the updated matrices\n",
    "        lora_alpha=64,  # parameter for scaling\n",
    "        target_modules=modules,\n",
    "        lora_dropout=0.1,  # dropout probability for layers\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        \n",
    "    )\n",
    "     \n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit=False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train \n",
    " pre-process the dataset and load the model using the set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1d7296c98a4930a335bea07f35750f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\" \n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 4096\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b76c5e259034de59ac2f1680eb5e929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400c824cbd0c427393f3ce95732be46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Preprocess dataset\n",
    "max_length = get_max_length(model)\n",
    "\n",
    "seed=0\n",
    "\n",
    "dataset = preprocess_dataset(tokenizer, max_length, seed, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop Training - not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import prepare_model_for_kbit_training, find_all_linear_names, create_peft_config, get_peft_model\n",
    "from peft.utils import print_trainable_parameters\n",
    "\n",
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            max_steps=50,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "     \n",
    "    do_train = True\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def loop_train(model, tokenizer, dataset, output_base_dir, num_iterations=10):\n",
    "    for i in range(1, num_iterations + 1):\n",
    "        output_dir = f\"{output_base_dir}/iteration_{i}\"\n",
    "        print(f\"\\nStarting iteration {i}/{num_iterations}...\")\n",
    "        train(model, tokenizer, dataset, output_dir)\n",
    "\n",
    "# Example usage:\n",
    "# Replace these with your actual model, tokenizer, and dataset\n",
    "model = model\n",
    "tokenizer = tokenizer\n",
    "dataset = dataset\n",
    "\n",
    "output_base_dir = \"results/llama2\"\n",
    "loop_train(model, tokenizer, dataset, output_base_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OG TRaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all params: 3,620,343,808 || trainable params: 79,953,920 || trainable%: 2.208462075433914\n",
      "torch.float32 382341120 0.10560906374558336\n",
      "torch.uint8 3238002688 0.8943909362544167\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5e9e4f5ea6459c8e2813026676de0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7637, 'grad_norm': 1.8639839887619019, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
      "{'loss': 2.0055, 'grad_norm': 2.7094788551330566, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.7581, 'grad_norm': 1.9332787990570068, 'learning_rate': 0.00019583333333333334, 'epoch': 0.0}\n",
      "{'loss': 1.3219, 'grad_norm': 5.164799690246582, 'learning_rate': 0.00019166666666666667, 'epoch': 0.0}\n",
      "{'loss': 1.5548, 'grad_norm': 1.4475767612457275, 'learning_rate': 0.0001875, 'epoch': 0.0}\n",
      "{'loss': 1.3765, 'grad_norm': 1.503376841545105, 'learning_rate': 0.00018333333333333334, 'epoch': 0.0}\n",
      "{'loss': 1.0875, 'grad_norm': 1.334741473197937, 'learning_rate': 0.0001791666666666667, 'epoch': 0.0}\n",
      "{'loss': 1.3103, 'grad_norm': 1.556593656539917, 'learning_rate': 0.000175, 'epoch': 0.0}\n",
      "{'loss': 1.2229, 'grad_norm': 3.6230978965759277, 'learning_rate': 0.00017083333333333333, 'epoch': 0.0}\n",
      "{'loss': 1.1501, 'grad_norm': 2.1270370483398438, 'learning_rate': 0.0001666666666666667, 'epoch': 0.0}\n",
      "{'loss': 1.1858, 'grad_norm': 2.1183724403381348, 'learning_rate': 0.00016250000000000002, 'epoch': 0.0}\n",
      "{'loss': 1.1217, 'grad_norm': 1.2902730703353882, 'learning_rate': 0.00015833333333333332, 'epoch': 0.0}\n",
      "{'loss': 0.9565, 'grad_norm': 1.3863322734832764, 'learning_rate': 0.00015416666666666668, 'epoch': 0.0}\n",
      "{'loss': 1.0493, 'grad_norm': 3.0798990726470947, 'learning_rate': 0.00015000000000000001, 'epoch': 0.0}\n",
      "{'loss': 1.1814, 'grad_norm': 1.378005862236023, 'learning_rate': 0.00014583333333333335, 'epoch': 0.0}\n",
      "{'loss': 1.0951, 'grad_norm': 1.435086727142334, 'learning_rate': 0.00014166666666666668, 'epoch': 0.0}\n",
      "{'loss': 0.8147, 'grad_norm': 1.2801687717437744, 'learning_rate': 0.0001375, 'epoch': 0.0}\n",
      "{'loss': 1.0985, 'grad_norm': 0.6429139375686646, 'learning_rate': 0.00013333333333333334, 'epoch': 0.0}\n",
      "{'loss': 1.3297, 'grad_norm': 1.2383564710617065, 'learning_rate': 0.00012916666666666667, 'epoch': 0.01}\n",
      "{'loss': 0.8173, 'grad_norm': 0.8662446737289429, 'learning_rate': 0.000125, 'epoch': 0.01}\n",
      "{'loss': 1.184, 'grad_norm': 1.0834343433380127, 'learning_rate': 0.00012083333333333333, 'epoch': 0.01}\n",
      "{'loss': 1.0624, 'grad_norm': 1.2298674583435059, 'learning_rate': 0.00011666666666666668, 'epoch': 0.01}\n",
      "{'loss': 0.9024, 'grad_norm': 0.9329010844230652, 'learning_rate': 0.00011250000000000001, 'epoch': 0.01}\n",
      "{'loss': 1.1443, 'grad_norm': 0.8301613926887512, 'learning_rate': 0.00010833333333333333, 'epoch': 0.01}\n",
      "{'loss': 1.3171, 'grad_norm': 0.8422142267227173, 'learning_rate': 0.00010416666666666667, 'epoch': 0.01}\n",
      "{'loss': 1.4986, 'grad_norm': 0.9109514355659485, 'learning_rate': 0.0001, 'epoch': 0.01}\n",
      "{'loss': 0.6319, 'grad_norm': 1.2568918466567993, 'learning_rate': 9.583333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 1.1421, 'grad_norm': 0.9873177409172058, 'learning_rate': 9.166666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 0.9784, 'grad_norm': 1.1824339628219604, 'learning_rate': 8.75e-05, 'epoch': 0.01}\n",
      "{'loss': 1.1671, 'grad_norm': 1.4048672914505005, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 0.9033, 'grad_norm': 1.184057354927063, 'learning_rate': 7.916666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 0.8326, 'grad_norm': 1.0441060066223145, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 0.803, 'grad_norm': 0.8914740085601807, 'learning_rate': 7.083333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 0.8654, 'grad_norm': 1.050278902053833, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 1.411, 'grad_norm': 1.0107170343399048, 'learning_rate': 6.25e-05, 'epoch': 0.01}\n",
      "{'loss': 1.0645, 'grad_norm': 1.3012733459472656, 'learning_rate': 5.833333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 1.279, 'grad_norm': 1.0799124240875244, 'learning_rate': 5.4166666666666664e-05, 'epoch': 0.01}\n",
      "{'loss': 1.067, 'grad_norm': 1.0885961055755615, 'learning_rate': 5e-05, 'epoch': 0.01}\n",
      "{'loss': 1.2635, 'grad_norm': 0.944398045539856, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 1.26, 'grad_norm': 0.9334511160850525, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 1.0038, 'grad_norm': 1.0845332145690918, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 0.989, 'grad_norm': 1.4149045944213867, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 0.6835, 'grad_norm': 0.9731258749961853, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 1.309, 'grad_norm': 1.0473101139068604, 'learning_rate': 2.5e-05, 'epoch': 0.01}\n",
      "{'loss': 1.322, 'grad_norm': 0.9388856291770935, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 1.2849, 'grad_norm': 0.772097110748291, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 0.8977, 'grad_norm': 1.6364970207214355, 'learning_rate': 1.25e-05, 'epoch': 0.01}\n",
      "{'loss': 1.0299, 'grad_norm': 0.9391890168190002, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.01}\n",
      "{'loss': 1.3422, 'grad_norm': 0.9207016825675964, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.01}\n",
      "{'loss': 1.3375, 'grad_norm': 0.9309670925140381, 'learning_rate': 0.0, 'epoch': 0.01}\n",
      "{'train_runtime': 703.9087, 'train_samples_per_second': 0.284, 'train_steps_per_second': 0.071, 'train_loss': 1.163570671081543, 'epoch': 0.01}\n",
      "***** train metrics *****\n",
      "  epoch                    =       0.01\n",
      "  train_loss               =     1.1636\n",
      "  train_runtime            = 0:11:43.90\n",
      "  train_samples_per_second =      0.284\n",
      "  train_steps_per_second   =      0.071\n",
      "{'train_runtime': 703.9087, 'train_samples_per_second': 0.284, 'train_steps_per_second': 0.071, 'train_loss': 1.163570671081543, 'epoch': 0.01}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "def train(model, tokenizer, dataset, output_dir):\n",
    "    # Apply preprocessing to the model to prepare it by\n",
    "    # 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    # 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Get lora module names\n",
    "    modules = find_all_linear_names(model)\n",
    "\n",
    "    # Create PEFT config for these modules and wrap the model to PEFT\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print information about the percentage of trainable parameters\n",
    "    print_trainable_parameters(model)\n",
    "    \n",
    "    # Training parameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=TrainingArguments(\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_steps=2,\n",
    "            max_steps=50,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "    \n",
    "    ### SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "    # Verifying the datatypes before training\n",
    "    \n",
    "    dtypes = {}\n",
    "    for _, p in model.named_parameters():\n",
    "        dtype = p.dtype\n",
    "        if dtype not in dtypes: dtypes[dtype] = 0\n",
    "        dtypes[dtype] += p.numel()\n",
    "    total = 0\n",
    "    for k, v in dtypes.items(): total+= v\n",
    "    for k, v in dtypes.items():\n",
    "        print(k, v, v/total)\n",
    "     \n",
    "    do_train = True\n",
    "    \n",
    "    # Launch training\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    if do_train:\n",
    "        train_result = trainer.train()\n",
    "        metrics = train_result.metrics\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "        print(metrics)    \n",
    "    \n",
    "    ###\n",
    "    \n",
    "    # Saving model\n",
    "    print(\"Saving last checkpoint of the model...\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "    \n",
    "    # Free memory for merging weights\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "output_dir = \"results/llama2/final_checkpoint\"\n",
    "train(model, tokenizer, dataset, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " save to a new directory and associated tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results/final_merged_checkpoint\\\\tokenizer_config.json',\n",
       " 'results/final_merged_checkpoint\\\\special_tokens_map.json',\n",
       " 'results/final_merged_checkpoint\\\\tokenizer.json')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#offload_dir = \"C:\\Users\\spite\\Documents\\FT-HR-Llama2\\Dataset_1_Setup [Dolly]\\models\"\n",
    "#model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16, offload_folder='./offload_folder')\n",
    "#model = model.merge_and_unload()\n",
    "\n",
    "output_merged_dir = \"results/final_merged_checkpoint\"\n",
    "os.makedirs(output_merged_dir, exist_ok=True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "\n",
    "# save tokenizer for easy inference\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(output_merged_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clears cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
