{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d032c958bce544368e2a4eee69ef7e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Replace 'your_model_directory' with the path to your model's directory\n",
    "model_name_or_path = r\"C:\\Users\\spite\\Documents\\FT-Llama2-HR_Chatbot\\results\\final_merged_checkpoint\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model(question, passage, max_length=200):\n",
    "    inputs = tokenizer.encode(question, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response_start = response.find(\"### Response:\") + len(\"### Response:\")\n",
    "    response_end = response.find(\"### End\")\n",
    "    \n",
    "    # Clean up any leading or trailing whitespace or newline characters in the response\n",
    "    final_response = response[response_start:response_end].strip()\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_model(question, passage, max_length=200):\n",
    "    \"\"\"\n",
    "    Generate a model response based on a question and a passage provided.\n",
    "    Format the output to extract the part between \"### Response:\" and \"### End\".\n",
    "    \n",
    "    :param question: The question to ask the model.\n",
    "    :param passage: The passage context for the model.\n",
    "    :param max_length: Maximum length of the generated response.\n",
    "    \"\"\"\n",
    "\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    # Encode the question and passage to the model's required input format\n",
    "    inputs = tokenizer.encode(question, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1)\n",
    "    \n",
    "    # Decode the generated response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the response part between the designated markers\n",
    "    response_start = response.find(RESPONSE_KEY) + len(RESPONSE_KEY)\n",
    "    response_end = response.find(END_KEY)\n",
    "    \n",
    "    # Clean up any leading or trailing whitespace or newline characters in the response\n",
    "    final_response = response[response_start:response_end].strip()\n",
    "    \n",
    "    return final_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"context\": \"User Jayden has been involved in the following projects: Vertex where they worked week1: 18hrs, week2: 15hrs, week3: 19hrs.\",\n",
    "        \"instruction\": \"Can you provide the total hours Jayden worked on Vertex?\",\n",
    "        \"response\": \"Jayden worked on Vertex for a total of 52 hours over 3 weeks.\",\n",
    "        \"category\": \"summarization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"context\": \"User Abigail has been involved in the following projects: Quest where they worked week1: 12hrs, week2: 13hrs.\",\n",
    "        \"instruction\": \"Can you provide the total hours Abigail worked on Quest?\",\n",
    "        \"response\": \"Abigail worked on Quest for a total of 25 hours over 2 weeks.\",\n",
    "        \"category\": \"summarization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"context\": \"User Dean has week1: 9hrs, week2: 8hrs.\",\n",
    "            \"instruction\": \"Calculate the total hours Dean worked. 9hrs + 8hrs\",\n",
    "            \"response\": \"Dean worked a total of 17 hours.\",\n",
    "            \"category\": \"addition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jayden worked a total of 35 hours on Vertex.\n"
     ]
    }
   ],
   "source": [
    "# Define the passage and question\n",
    "passage = \"User Jayden has been involved in the following projects: Vertex where they worked week1: 18hrs, week2: 15hrs, week3: 19hrs.\"\n",
    "question =  \"Can you Calculate the total hours and weeks Jayden worked on Vertex?\"\n",
    "\n",
    "# Call the updated ask_model function with both the question and passage\n",
    "response = ask_model(question,passage)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_name_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         response \u001b[38;5;241m=\u001b[39m ask_model(question)\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_name_\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_main_\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     15\u001b[0m     main()\n",
      "\u001b[1;31mNameError\u001b[0m: name '_name_' is not defined"
     ]
    }
   ],
   "source": [
    "def external_prompt():\n",
    "    return \"Welcome to ChatBot! Type 'exit' to quit.\\n\"\n",
    "\n",
    "def main():\n",
    "    print(external_prompt())\n",
    "    while True:\n",
    "        question = input(\"You: \")\n",
    "        if question.lower() == 'exit':\n",
    "            print(\"ChatBot: Goodbye!\")\n",
    "            break\n",
    "        response = ask_model(question)\n",
    "        print(f\"ChatBot: {response}\")\n",
    "_name_ = \"_main_\"\n",
    "if _name_ == \"_main_\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        question = input(\"Ask a question: \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        response = ask_model(question)\n",
    "        print(\"Answer:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
